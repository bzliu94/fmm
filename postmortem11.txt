2019-10-25

POSTMORTEM #11

1. We have Xue et al. 2018 approach for partially-inside exact range closest-pair query for d dimensions that involves epsilon; we let d be two. If we have two straight diagonal staircases, epsilon needs to be in O(1 / n), which makes the approach too expensive. If we use uneven spacing (e.g. via having pair of staircases that looks like rectangular hyperbola -- one portion in quadrant I and another portion in quadrant III), symmetric case is handled well. By symmetric we mean we choose A vector and B vector that have point locations that are horizontally equally distant from origin. But, if we consider an asymmetric pair, it is likely that we are forced to have our query range be contaminated by a neighbor vector (i.e. this happens for vector out of two that is closer to origin horizontally). If we think that we can chop up so that one dimension is devoted to left staircase and a second dimension is devoted to right staircase, we likely have too much space required because of necessary associated cloning. It is important that we use two dimensions in total to handle selection of both A vector and B vector and not e.g. four dimensions; this is the case even if we want to have more dimensions so that we can make sure asymmetry w.r.t. selecting A vector and B vector is not an issue (s.t. epsilon would not be too expensive) given that we pursue partially-inside query.

2. We briefly return to Kaplan et al. 2008. We think we can avoid color carpentry by using 4-d orthants and point neutralizers. We have orthant difference for each well-formed query (i.e. a 4-sided query including exactly one A vector and one B vector). But, this only works when the left orthant is fixed (i.e. A vector is fixed). Otherwise, the neutralizing may be undesirable given that left orthant perspective may be that we have either a zero or a non-zero for a color instead of always a zero or always a non-zero.

3. We have hypersphere approach based on Oh/Ahn 2018, again. It turns out that a hypersphere of radius one maxes out on surface area at d ~= 7. The main idea is that surface area grows nearly exponentially before it tops out. The problem is that we necessarily deal with low target dimension and thus moderate values of n (but still upper-bounded by a constant), which means big-O notation is a little bit abused. We aim to describe the time by how much it changes the power of the polylog factor. We call this strategy an "opinionated" interpretation. However, in the end, we experimented with seeing how we would change the power of the base-n factor and it ended up being too much -- it would after including a base power of two be greater than or equal to three (i.e. four). Considering re-writing this effect by changing polylog factor power seems dishonest, then. Actually, the main problem is Oh/Ahn 2018 coefficient (i.e. (1 / epsilon) ^ (d - 1)); it's very hard to be clever with such a large coefficient power. If epsilon is in O((1 / (n ^ 2)) ^ (1 / (d - 1))), coefficient involving epsilon becomes n ^ 2. (Thus, a power of two plus two, which is four.) This is independent of optimistic assumption that we have perfect exponential growth in surface area s.t. at d == 2 we support storing n points that are regularly spaced apart (not assuming every pair is same angle apart) and each increase in dimension by one increases number of points we can store by exactly a factor of n. In other words, irrespective of the interesting phenomenon that surface area for a hypersphere with radius of one maxing out at d ~= 7, a quirk of the Oh/Ahn 2018 algorithm is to undeniably make the algorithm for our application be too slow.

Also, we should note that we consider how to pack points onto surface of a hypersphere via Lovisolo/da Silva 2001. We have some tolerance for imperfect packing. Also, we consider letting point generation for a fixed n be done once and have this time be separate from the time for running the range diameter query algorithm. The angle estimate would be min. of angles used for each of d - 1 angular dimensions and we can easily modify the algorithm to only generate points in positive orthant. We also consider that for d in [2, 7], exact packing ratio for hyperspheres (not necessarily on surface of a higher-dimensional hypersphere) is known, which helps us assess impact on theoretical time.

Two important details that modulate estimate for time based on assumption of uninterrupted exponential growth in surface area as dimension increases are value of gamma in denominator for surface area for a hypersphere and how well we pack w.r.t. perfect packing (i.e. ratio of one).

4. We consider Shamir's approach, which promises Boolean matrix multiplication with O(r * n ^ (2 + 1 / r)) time. For example, if r is three, then time is O(3 * n ^ 2.33) = O(n ^ 2.33). The use of Chinese remainder theorem is to make sure that we have r moduli chosen that are each around n ^ (1 / r). This is so that the product of the moduli matches or slightly exceeds n; then, we can use the residues to re-construct the value for an output element. Or, if we plan to support only values of zero or non-zero for BMM application, we check that all residues are zero -- if they are, then the output is zero. But if we have r subproblems, we tend to have size of a subproblem of O(n ^ 3 / r). So, even though n ^ (1 / r) is small (i.e. smaller than n / r asymptotically for r > 1), we are feeding in values to get mod of via subproblems that take O(n ^ 3 / r) time to process. The time then is O(r * n ^ 3 / r) = O(n ^ 3), which is too much. Alternatively, if we assume each subproblem is smaller -- i.e. size O(n ^ (2 + 1 / r)), then we have O(n / n ^ (1 / r)) number of subproblems; total time required is their product, which is O(n ^ 3).

Going back to Shamir's approach -- they prefer to play with indices, but to consider the number of residues multiplied by non-residues, we have n again, so time is O(n ^ 2 * n) = O(n ^ 3). Also, it seems that they do not have enough combinatorial complexity unless we multiply n ^ 2 by n, because we are creating two-level hierarchy instead of using a one-level hierarchy s.t. number of combinations handled is O(n) in both cases; otherwise we can say that we have "collision" when we map to same top-level (associated with non-residue for after transform) node (i.e. many situations that are associated with different combinations map to same node s.t. we loose important details).

5. Han approach from arxiv. It is currently unclear to us if their approach will work. We note that a time of O(n ^ 2 * log(log(n)) ^ (log(log(n)) ^ 2) is dominated by n ^ epsilon. So, if that time estimate is valid, the approach could be desirable. We found a bug (or at least ambiguity) in equation one in version nine of their article, so we are stopping investigating it unless the author emails us back. The main idea is to determine an output dot product by describing the problem as a product of two short polynomials which we then compute using FFT. It uses re-writing by separating out summations, uses proofs involving integrals, and a step involves the solving of a Vandermonde system of equations.

--

References

* Xue et al. - Approximate range closest-pair search (2018)
* Kaplan et al. - Efficient colored orthogonal range counting (2008)
* Davoodi et al. - Two-dimensional range diameter queries (2012)
* Oh and Ahn - Approximate range queries for clustering (2018)
* Lovisolo and da Silva - Uniform distribution of points on a hyper-sphere with applications to vector bit-plane encoding (2001)

Controversial/incorrect references

* Han - An O-tilde(n ^ 2) time matrix multiplication algorithm (2019)
* Shamir - Almost optimal Boolean matrix multiplication by multi-encoding of rows and columns (2018)

--


