2019-09-06

Postmortem #6

APPROACH #1

We came across an article that seems to have quite a bit of muscle, but ends up not being enough because resolution is expensive by way of epsilon needing to be a function of n, which leads target dimension to be too large for our application. Our application is the problem of solving Boolean matrix multiplication of two square matrices s.t. the semiring is OR-AND s.t. side length is n. We call this lax Boolean MM. We can approach the problem by determining if dot product of two vectors is zero or non-zero. One important place where we deviate from a proof that shows that the dot products for after dimension reduction have small enough additive error (i.e. epsilon of 0.5 s.t. it is roughly independent of n) is that norms of vectors used to be assumed to be normalized to length one.

Dadush et al. 2018 provides a way to perform Johnson-Lindenstrauss transform deterministically and efficiently -- i.e. the transform is normally randomized but now can be performed so that we succeed on our first try. The derandomization uses pessimistic estimators. An important property is that norms are preserved using an additive error. The approach is input-dependent -- i.e. is designed to guarantee norms for the specific vectors provided. Normally, using norm preservation we can show inner product (of which a special case is dot product) is preserved using additive error, as well. It is unclear whether the input-dependent restriction still allows inner product for a pair of input vectors to be preserved using the same or a similar bound; to prove it we would have to use not just (assuming u and v are input vectors) e.g. norm of u + v or norm of u - v. Assuming this detail can be resolved, then another hitch is that we assume that the input vectors are normalized to be length one; a straightforward remedy when we care about knowing when dot product is zero to enough confidence even as n grows means epsilon is not likely to be boundable by a constant. If epsilon is in 1 / sqrt(n); epsilon is part of formula for target dimension m; m = O(12 * ln(2 * n) / (epsilon ^ 2)); then, m could be in O(log(n) * n), which is super-polylogarithmic and thus is too large. (Alternatively, if epsilon is bounded by a constant, then m can be back to in O(log(n)), which is good.)

A U. Chicago discussion section proof of inner product preservation requires assumption that |u| and |v| are upper-bounded by one. Alternatively, if we only require knowing that the dot product is non-zero, perhaps that is a good lead. Dot product is largest with small angle (i.e. zero degrees) and is smallest with large angle (i.e. large is at most 90 degrees -- i.e. orthogonal). We opt to consider geometric interpretation of dot product. Instead of caring about being perfectly orthogonal or parallel (noting that we cannot be e.g. anti-parallel with the assumptions we are making so far), perhaps we care about being within 45 degrees of perfectly parallel; then, epsilon can still be bounded by a constant because cos(45 degrees) is related to cos(0 degrees) by being fraction equal to (sqrt(2) / 2) / 1 = sqrt(2) / 2 ~= .707, which is lower-bounded by a constant. It is unclear how we make use of knowing whether original dot product is non-zero assuming angle is within 45 degrees. It seems we may have a situation on our hands similar to bucket sort with way too many buckets to check (i.e. we have an approach that normally would seem sound). It is important to point out that at this point it is normal to point out that we need to know if angle is too large, we do not fail quietly -- i.e. we need to know when to trust the modified dot product and when to not. Cosine for small angle (i.e. close to zero) changes in smaller increments than cosine for angle around pi / 2. This means that given fixed angular incrementing that e.g. if we have good error for case of angle equal to 45 degrees that since increments grow in size as we approach 90 degrees from there that we need not decrease error and thus need not change epsilon in a way that adversely affects m. We could get lost in this rabbit hole, but we note that to get good error for 45 degrees is already a big assumption because it's still a function of n; we must not conflate the niceness of dot product within a constant factor of and being close to n ^ 2 (s.t. n ^ 2 is associated with perfectly parallel) with the impact on epsilon to get the error we would need to tell difference between current number of one bits in a vector and adding one more one bit to or subtracting one bit from a vector (all the while assuming that vectors start off with components in {0, 1} and later get normalized to length n). We also note that cosine around small angle x behaves like 1 - (x ^ 2) / 2 -- i.e. related to O(x ^ 2). Also, angle x grows when we add a one bit or remove a one bit from a vector that has not been normalized to length n s.t. we do roughly sublinearly in n; this means that as n increases we do not increase fine-ness in angle as fast as linear. Then, we may think that we can counteract the quadratic fine-ness required by increasing efficiently-obtainable fineness by factor of O(n ^ 2), which we fallaciously assume that since we scale vectors' norms by n each that we can increase resolution for a dot product of vectors a and b by factor of |a| * |b| = n ^ 2 while keeping the version of epsilon that we use to calculate cost at some constant between 0 and 1 (say e.g. 0.5). We may think that the resolution can be increased for same cost because the factors |a| and |b| are known exactly. This does not make sense because by metaphor a pen-sized drawing scaled up by even an exact amount (e.g. say 100) still has errors that are scaled up by that same factor as well, not pen-sized errors again. Finally, say we wish to rotate vectors so that we have better chance of guaranteeing in small number of combinations of two vectors s.t. each may be original or rotated that at least one case has angle within 45 degrees of perfectly parallel (i.e. within 45 degrees of zero degrees). We will not yet say how a useful rotation can be obtained. First, we need to identify when the angle is small s.t. we can trust it as opposed to too large, where the error will be too large for affordable epsilon; we need to not have failures be quiet. We ignore fact that this is not doable s.t. target dimension m is small enough for our application; assuming we support telling difference between adding and removing a one bit to pre-n-normalized vector at angle exactly equal to 45 degrees, this basically means we get knowing exact modified dot product for angle in [45 degrees, 90 degrees] because the increments only grow, while epsilon remains the same. This is becoming nonsensical.

Say we rotate. As an indicator of how much of a rabbit hole we have made of considering this JL-transform-based tentative approach, we again assume that we only trust a modified dot product if angle is in [0 degrees, 45 degrees]. (Note that we just said that if we have perfect ability to tell between adding a one bit or subtracting a one bit to a pre-n-normalized vector, we have suitable error to also know for [45 degrees, 90 degrees], making the compatible range into [0 degrees, 90 degrees], the full extent of possible angles.) For the purpose of conveying the ideas that have passed through our head, we note that if we only treat modified dot product as useable when angle is in [0 degrees, 45 degrees] (without jumping to assuming we know modified dot product finely for full range of [0 degrees, 90 degrees]), we may wish to rotate through main diagonal of the unit hypercube to force angle to be smaller. We may even wish to do this not just for one of two vectors but for both original vectors in a pair. Say we rotate through main diagonal an angle equal to that between origin-rooted-main-diagonal and point on axis-aligned wall for hypersphere octant (noting that we use hypersphere and hypercube interchangeably s.t. for former we assume vectors are normalized and for latter we are not yet) s.t. these two points are coplanar with source vector. We even believe that if we rotate once then we have a chance of reducing angle and of having two vectors be within pi / 4 of each other; or, if that rotate is not enough, then we could either only rotate the other vector or rotate both vectors. For the four possibilities of <original, original>, <original, rotate>, <rotate, original>, <rotate, rotate>, perhaps at least one can be guaranteed to have angle within pi / 4 and we can know exactly when to ignore the cases where angle is not within pi / 4. Well, the rotate we have described is not as interesting as we once believed -- these original vectors we have rooted at origin and thus will always end on some axis-aligned wall of the hypersphere octant; so, when we rotate some vector using the definition we have provided for rotation, this vector we rotate will always end up exactly on the main diagonal; we do not move a vector past the main diagonal ever. This is important because then we introduce idea of "negative angle", which we use by saying that we get some conclusion about modified dot product from two vectors (either of which may be rotated), which we relate to some original dot product by un-rotating. This means we might need to know exact angular relationship between that for two possibly rotated vectors and that for original two vectors, which we did not ever figure out. Say we did have some similar transform s.t. we know how angles transform (and we can do this efficiently) when un-rotating; then we might make use of an upper bound on angle we can add by un-rotating. Say we rotate one vector only out of two and we know then that an angle becomes within 45 degrees; then we may have an upper bound on recoverable angle of 45 degrees and thus we know we could have been orthogonal for original pair of vectors. Say we rotate both vectors and we know we are within e.g. 30 degrees of each other; then if we have an upper bound of 60 degrees recoverable when we un-rotate each vector, then we could have been orthogonal for original pair of vectors. This nebulous idea of negative angle would then presumably come into play to measure to what degree when we move a vector by rotating it we start to move away from each other after a period of moving closer to each other; if we have negative angle with these particular upper bounds on recoverable angle, then we may know with some degree of certainty that the original two vectors must not have been orthogonal to each other. Of course, these calculations must also be efficient. In the end, this sub-approach that makes use of rotations somewhat falls apart because we again are not able to have enough resolution efficiently.

There are five remaining details. First, as an aside, angle between farthest edge in hypercube and main diagonal approaches pi / 2 as n approaches infinity. Second, we assume an input vector is not ever a zero-vector; in other words, we treat that case separately. Third, floating-point imprecision is an issue, as well. Fourth, we considered how to rotate a predictable amount in higher dimensions s.t. for a pair of result vectors we know tight angle bound for we also know original angle for; one way we consider is to rotate 90 degrees CCW for every pair of dimensions assuming we do not encounter a 2-d zero-vector and assuming n is even; then, if n is actually odd, our response is ambiguous -- perhaps we pad to make sure n is even; we do not have concrete plans for this rotation approach. Fifth, for the proof from U. Chicago, we use fact that a vector inner-producting itself (i.e. via dot product specifically) is equal to its norm squared. This follows via Pythagorean theorem or distance formula.

APPROACH #2

There is an article from Harvey and van der Hoeven 2018 that has a title that makes it seem relevant, but is not for our application. The proof starts at section three describing pseudocode for integer matrix multiplication; there is a low-level justification that involves a time expression that is both partially specified (i.e. it does not mention a base case) and that is recursive. Note that the approach is divide and conquer except that instead of having for subproblems a smaller size for the matrix, we have fewer bits for each element. Then, the main theorem that came before in section one is proved in section four; it serves as the closed-form time for the algorithm given in section three. The time ends up being related to omega for an existing FMM algorithm. Alternatively, we can drop factors involving d (which include those involving omega) if n is said to be much larger; so, we get good time for a case that we are not interested in -- that n >> d (noting that n is number of bits per input element and that d is size of side for an input matrix; these names mean what they do because that is what is assumed in this particular article). In the end, the algorithm still ends up being at least cubic time if n is not >> d, as is the case for our application. To their credit, they do have some other applications identified where n can be >> d.

APPROACH #3

Grolmusz 2003 talks about handling matrix multiplication efficiently assuming we only care about output values s.t. we take modulo of them using a small constant (e.g. six). Modulo using six or using a larger modulus is not relevant because presumably the larger the modulus the more time we take; besides, in a separate article (i.e. Grolmusz 2008), he says they have not solved general matrix multiplication efficiently; it reminds us of using Chinese remainder theorem; even if we want to know difference between when output should be zero or non-zero, it's not clear how we would efficiently do so using only a few different and small moduli (noting that we do not have way to e.g. measure having modulus of two with zero offset and having modulus of two with offset of one).

APPROACH #4

Kaban 2015 talks about better bound for inner product preservation for Johnson-Lindenstrauss lemma. However, while they do not require norms be less than or equal to one for additive error, they do say additive error from epsilon is scaled up by the two norms of the associated two vectors, which means we essentially have the same problem as we had before.

References

* Dadush et al. - Fast, deterministic and sparse dimensionality reduction (2018)

* Kakade and Shakhnarovich - Lecture #2: Random projections (2009)
https://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf

* Harvey and van der Hoeven - On the complexity of integer matrix multiplication (2018)

* Grolmusz - Near quadratic matrix multiplication modulo composites (2003)

* Grolmusz - Modular representations of polynomials: hyperdense coding and fast matrix multiplication (2008)

* Kaban - SLIDES: Improved bounds on the dot product under random projection and random sign projection (2015)
http://www.cs.bham.ac.uk/~axk/kdd15-slides.pdf

* Kaban - Improved bounds on the dot product under random projection and random sign projection (2015)

--


