2018-12-06

Postmortem

We exhausted our basic ideas. That means ideas not involving tensors.

Attempted approaches for (plus, times) for strict Boolean:

1. Extend Kaplan 2008 staircase pair with a 4-d segment tree with dimensions x, y, p, z. z is associated with A row prefix and p is associated with B column. We make up for using low-space with three-sided queries by having telescoping. Later, we can drop x and y to get better time. The issue is that telescoping is difficult to make compatible with saturation efficiently. We use color carpentry and have expiry rectangles; for A rows at left, we always count one bit contributions, but we must introduce "expiry rectangles" that activate for a fixed color for various z, but only for when we also have a one bit for a target p. This could be "correct", but requires too much space. Alternatively, we can use a rectangle for each color (i.e. we have n of them) with associated p and z values. To still use a 4-d (or later 2-d) segment tree, we assume the ranges are not gappy but solid initially, but then we need to do set intersection that may take quadratic time per node, which implies time cubic in n. If our tree is balanced, non-quadratic-time-saturated nodes exist but we grow from time in n for a node to n ^ 2 exponentially fast, so we don't have much time lowering from power of three. If our tree is linear, non-quadratic-time-saturated nodes exist but we grow from time in n for a node to n ^ 2 quadratically fast, which still leaves a lot (i.e. O(n - sqrt(n))) of expensive nodes.

2. If we could merge hash tables that store unique integer keys with integer stored counts as values in time sublinear in the number of items in the hash tables, not assuming keys for the two operand hash tables are disjoint, we would be able to do set difference quite well and use p/z telescoping for overall time subcubic in n. We might wish to use e.g. a balanced binary tree (e.g. a red-black tree). Or, we considered using prime-based hash table -- the hash table is a product of primes raised to powers -- each prime is a key and each power is a value; however, while combining the hash tables seems simple (i.e. just multiply), the amount of space required is at least linear in the number of key values, which does not help for time for combining, even if we make an effort to use small primes.

3. We take advantage of nearest neighbor, assuming l1-norm distances on average are at worst sublinear in n, which is not true -- such distances on average are at worst in O(n). We note that for one input matrix we have n vectors that each are in n dimensions. We use max-weight minimum spanning tree edge weight to upper bound l1-norm distance between vectors on surface of an n-dimensional hypercube (via packing of n primitive (n - 1)-dimensional hypercube volumes given that packing constant for hyper-rectangles is generally one). We use relationship w.r.t. length protrusion between n-dimensional hypercubes and hyperspheres to help us re-write in terms of each other efficiently. We use approximate nearest neighbor algorithm to get good time guarantee e.g. via LSH and Johnson-Lindenstrauss lemma and turn these into Las-Vegas-type, given that we know an upper bound for distance that must be satisfied. However, the predicted distance is not favorable. We have analysis of d-simplex and existence of Hamiltonian paths for graph defined by vertices and edges of that polytope (for purpose of showing that when number of points in n-dimensional space is more than n + 1, while we are not mutually equidistant, a prediction based on packing is still valid when it comes to creating associated MST and l1-norm distances can be considered to be steadily decreasing). We use proof that vertices of a regular d-simplex lie on the surface of a hypersphere (which validates the idea of considering distortion when relating distances that assume a primary n-dimensional hypersphere to those for our primary n-dimensional hypercube). We should note that we get good asymptotic results for average distance for fitting primitive volumes into overall shape surface area if we use (n - 1)-dimensional hypercube primitive (as we have mentioned). However, if we alternately would like to consider (n - 1)-dimensional hypersphere primitive, we may arrive at prediction of huge l2-norm distance (and subsequently l1-norm distance); then, packing constant plummets for large dimension -- if we knew exactly the constant based on n, the effect of the packing constant is to reduce radius and it is conceivable that we might get lower upper bound w.r.t. if we use hypercube primitive. We opt to avoid hypersphere primitives altogether, however, given that we are uncertain about packing constant in terms of dimension. For hypercube primitive, we get side length (independent of dimension) in O(1). Then, going from l2-norm to l1-norm, we multiply by O(n) to get O(n). For hypersphere primitive, if we have radius (independent of dimension) in l2-norm, we multiply by O(sqrt(n)) to go to l1-norm. The reason we invoke packing constant is that it levels the playing field by allowing us to take advantage of wasted space w.r.t. same side length or (almost equivalently) radius; for growing n (i.e. dimension) hypercube volume grows (indicating solidness) but hypersphere volume grows initially and later aggressively shrinks (indicating that we have space ripe for packing closer).

4. We use two-phase shuffling (i.e. sorting) or clumping. We start off with intention of using divide-and-conquer with output submatrices defined in terms of sums of products of input submatrices as with Strassen. We have 0/1-swapping in that for each submatrix we have option of replacing ones with zeroes or vice versa assuming that we favor one over the other as we exploit sparsity as part of our seeking roughly homogeneous "cleared" regions (i.e. in general we do accept either close to all zeroes or close to all ones in a region). We have upper-left quadrant clearing -- we show equivalence to other configurations of cleared rectangles. Two-phase shuffling is where we sort s.t. we clear a region (e.g. upper-left quadrant) as much as possible; i.e. we sort using some custom metric along x (keeping "columns" intact) and then along y (keeping "rows" intact). Issue is that with larger n, it becomes harder to clear regions proportionately via shuffling along x and then along y. This is related to law of large numbers. With clumping, we use minimum spanning tree with pre-order traversal to arrive at 2-approximation for TSP whereby the weights are number of one bits aligned. Specifically, we use complement of "asymmetric Hamming distance" -- in other words, we use n minus asymmetric Hamming distance. We say "asymmetric" because pair of one bits is associated with zero distance, but pair of zero bits is associated with one distance). We reduce TSP path to standard TSP (i.e. the flavor for cycle) efficiently. Given that this is essentially strict Boolean MM, we eventually plan to use approximate Boolean MM, which is more tractable, for the weighting. It turns out that we get worse quality for clearing rectangles with two-phase clumping (i.e. along x and then along y) than for two-phase shuffling. The idea is that we can initially attempt to clear regions in A matrix or B matrix in isolation. We remember that interaction between A and B can lead to product that is C via traditional (i.e. again as with Strassen, roughly) re-writing of C submatrices in terms of A and B submatrices. Then, given that colors for A and B are not independent, we view the input A rows and B columns as arranged (respecting each other's color orders) into one matrix that we can clear rectangles for and ignore submatrices s.t. we roughly ignore subproblems (or that subproblems are cheap to solve) and can reduce branching factor and possibly subproblem sizes, given that we cut our teeth on individual matrices already. The fatal flaw is that we have difficult time clearing regions as n grows, as we mentioned earlier.

5. We use "compensators" and truth tables. We know that we have issue with Kaplan 2008 approach because we have expiries that have too much detail and thus take too much time to use as part of pre-processing. We consider whether we can take advantage of truth table and introduce extra rectangles at particular locations with particular weights s.t. we can have same effect but automatically adjusted, which we believe may be plausible given that we are pursuing strength reduction s.t. we don't explicitly address over-counting or under-counting of one bits but rely on whatever changes we can imagine that only are required to make sense by the time we have a query s.t. then at query time we arrive at correct answer. The fatal flaw is that we, after considering when we have left zero or one bit and right zero or one bit that we don't reduce the size for each successive subproblem s.t. we might continue with compensators and compensator compensators and compensator compensator compensators, etc. in a never-ending manner. One approach involving this idea we estimate would require an extra factor of O(log(n) ^ n) for time, which of course is too much.

6. Use skew staircases. We extend Kaplan 2008 approach s.t. we don't have standard pair of staircases, but at right we have possibly points (i.e. images of rectangles) that may share same x. This segues into idea of storing "superimposed trees". For every log(n) groups of adjacent color values, we have possibly-truncated binary trees s.t. each branching corresponds to splitting into zero bit for a specific color or one bit for that color. Then, we have n / log(n) trees. It turns out that we don't need trees; we can just partition into two groups for each color independently. Skew staircases touch on idea that we can share work for each partition. Ultimately, this leads to idea of having grouping across islands for the superimposed trees via an extra component p s.t. we associate each B column with a unique p value. Then, given that we don't need superimposed trees, this leads to assigning each partition a set of z values and a set of p values. This leads to too much time for pre-processing as we need to do set intersection repeatedly, which implies (somewhat loosely) possibly time in O(n ^ 2) per node in a segment tree that may have O(n * log(n) ^ 2) nodes. Needless to say, this is too much because we then have time that is cubic in n.

7. Have multiple staircases at right. We have "rows of teeth" as with a shark. This way, we have low number of dimensions, but we are unable to share work across subtrees via cross edges for superimposed trees; if we use p values to combine across islands, we can live with p values and not need multiple staircases at right. The concern is that via both ways we have pre-processing time that is cubic in n.

8. This is not an isolated approach; a later step for any working approach that uses a segment tree is to shave off a log(n) factor for time by using fractional cascading and possible lowest-layer interval tree. For range tree, we just need fractional cascading for a similar effect (given that the space is already improved).

Attempted approaches for (plus, times) for f.p.:

1. We use word-packed version of (plus, times) for Boolean (i.e. strict Boolean). Then, we use number-theoretic FFT, bit spreading/gathering, and Parseval's theorem; each subproblem involves zeroes and ones, so we don't have inflation; square of one is one, so overall energy being conserved means overall number of bits is conserved; this means certain bins will require larger number of bits while other bins will have fewer number of bits. For numerical robustness, we might have to use run-length encoding and tiling.

2. We were intrigued by van der Hoeven 2012 saying that their integer MM case can be used for f.p. case via pre-processing that involves "modulo pre-conditioning". Presumably, this approach leads to numerical robustness in that we don't have catastrophic cancellation. Given that their paper was retracted, however, and that it seems that they did not go into that aspect in detail and that even if their approach was correct would have been difficult for us to understand -- we will not pursue this further for now.

Attempted approaches for (min, plus) or lax Boolean:

1. Have a point arrangement that has a hot-dog for A rows and a hot-dog for B columns s.t. the hot-dogs are stacked. We use closest pair range query with critical min. distance; we return a pair of points in a rectangular range that are at least a certain distance away from each other. This does not work because the article that we saw that mentions this query being supported quickly we misinterpreted and it assumes that all point pairs in the query rectangle consist of points that are far enough away from each other. An almost equivalent query that would be acceptable is range diameter query (i.e. for farthest-pairs in a range), but this seems to either require too much time for pre-processing or too much time for query. We could end up with power of 2.5 but end up with polylogarithm power that is at least five (via Gupta 2008); this seems like an improvement, but we decide not to pursue it. Besides, a common application for (min, plus) is lax Boolean, which we can reduce to integer (min, plus), for which there exist algorithms with theoretically better power, already.

2. Of course, given that lax Boolean (which is for a semiring) can be reduced to strict Boolean or integer (plus, times) cases (which are for rings), it is not required that we go for lax Boolean separately and independently. However, we also had planned on reducing lax Boolean to (min, plus) and (min, max) tropical semirings, each of which are more difficult to reduce to strict Boolean or integer (plus, times) ring cases. Given that our lax Boolean approach fell through, we considered instead opting to solve an implementation problem instead of going for theory. Specifically, we consider approaches that are close to quadrarithmic time. These include van der Hoeven 2012 and Han 2017. The former ended up being withdrawn with an explanation (involving details that would have been hard for us to come up with). The latter takes time that is not strictly in O(n ^ 2 * polylog(n)) -- i.e. it is more. Also, after peeking at the internals, it seems that it is generous to assume that we have constant factors of one for overall expression and for power. This implies that the approach would still not show improvement except again (i.e. compared with current state-of-the-art for MM) for huge n.

Miscellany:

1. Some results can already be made with power being 2.5; e.g. for (min, plus) via range diameter query and a point arrangement involving n ^ 2 points and a pre-processing/query time trade-off, we can, but the associated polylog factor power is around five, which is quite large and so we do not pursue it.

2. Davoodi 2012 mentions range diameter (i.e. farthest pair) query. They have a point arrangement involving concentric circles and that uses triangle inequality to be able to reduce effectively lax Boolean MM to that type of query. Their reduction is clever. They say either one can have polylogarithmic-time query or one can have space sub-quadratic in n. There is a tradeoff with power being 2.5 and we have polylogarithmic factors. Their point arrangement is similar in vein to our proposed hot-dog point arrangement that uses closest pair range query with critical min. distance. Gupta 2008 is the actual article detailing such a trade-off.

References

* Kaplan et al. - Efficient colored orthogonal range counting (2008)
* Gupta et al. - Data structures for range-aggregate extent queries (2008)
* Davoodi et al. - Two-dimensional range diameter queries (2012)

Controversial/incorrect references

* Van der Hoeven - Fast multiplication of integer matrices (2012) -- withdrawn
* Han - An n ^ 2 * (log(log(n)) ^ (log(log(n)) ^ 2)) time matrix multiplication algorithm (2017) -- uncorroborated

--


